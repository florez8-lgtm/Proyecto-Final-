# -*- coding: utf-8 -*-
"""Proyecto final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18NDrskcXlqnniSy3KOnZY1HCd_pnrgkq
"""

import kagglehub
import pandas as pd
import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Descargar el dataset
path = kagglehub.dataset_download("lakshmi25npathi/bike-sharing-dataset")
print("Dataset descargado en:", path)
# Cargar el archivo day.csv
df = pd.read_csv(os.path.join(path, "day.csv"))
# Vista inicial
print('Primeras 5 filas del dataset:')
print(df.head())

print('\nInformación general del dataset:')
print(df.info())

print('\nEstadísticas descriptivas del dataset:')
print(df.describe())

try:
    print(df.describe(include='all', datetime_is_numeric=True))
except TypeError:
    print(df.describe(include='all'))

print("\nValores faltantes por columna:")
print(df.isnull().sum())

duplicados = df.duplicated().sum()
print(f"\nNúmero de filas duplicadas: {duplicados}")

# Estandarizar nombres
df.columns = (df.columns
              .str.strip()
              .str.lower()
              .str.replace(' ', '_')
              .str.replace(r'[^a-z0-9_]+', '', regex=True))
print("\nColumnas estandarizadas:")
print(list(df.columns))

#  Parseo de fecha
if 'dteday' in df.columns:
    df['dteday'] = pd.to_datetime(df['dteday'], errors='coerce')
    print("\nTipo de dato de dteday:", df.dtypes['dteday'])

#Orden de columnas esperadas
expected_cols = [
    'instant','dteday','season','yr','mnth','holiday','weekday','workingday',
    'weathersit','temp','atemp','hum','windspeed','casual','registered','cnt'
]
present_cols = [c for c in expected_cols if c in df.columns]
df = df[present_cols]
print("\nColumnas en uso (orden esperado si existen):")
print(list(df.columns))

# Revision previa
if 'dteday' in df.columns:
    print("Duplicados por fecha (dteday):", df.duplicated(subset=['dteday']).sum())

for col in ['temp','atemp','hum','windspeed']:  # deberían estar en [0,1]
    if col in df.columns:
        out_low  = int((df[col] < 0).sum())
        out_high = int((df[col] > 1).sum())
        if out_low or out_high:
            print(f"Advertencia: {col} fuera de [0,1] -> <0: {out_low}, >1: {out_high}")

for col in ['casual','registered','cnt']:        # no deberían ser negativos
    if col in df.columns:
        negs = int((df[col] < 0).sum())
        if negs:
            print(f"Advertencia: valores negativos en {col}: {negs}")

# LIMPIEZA Y TRANSFORMACIÓN
df_clean = df.copy()

# Eliminacion de  duplicados
df_clean = df_clean.drop_duplicates()

# Eliminacion de  Duplicados por fecha
if 'dteday' in df_clean.columns:
    antes = len(df_clean)
    df_clean = df_clean.drop_duplicates(subset=['dteday'], keep='first')
    print("Eliminados por fecha duplicada:", antes - len(df_clean))

# Eliminacion de Filas con fecha inválida
if 'dteday' in df_clean.columns:
    antes = len(df_clean)
    df_clean = df_clean[~df_clean['dteday'].isna()]
    print("Eliminadas por dteday nulo/inválido:", antes - len(df_clean))

# Clip normalizados a [0,1] (recorte de valores para que queden dentro del rango)
for col in ['temp','atemp','hum','windspeed']:
    if col in df_clean.columns:
        df_clean[col] = df_clean[col].clip(0, 1)

#  No-negativos en conteos
for col in ['casual','registered','cnt']:
    if col in df_clean.columns:
        df_clean[col] = df_clean[col].clip(lower=0)

# Consistencia cnt = casual + registered (si ambas existen)
if set(['casual','registered','cnt']).issubset(df_clean.columns):
    cnt_antes = df_clean['cnt'].copy()
    df_clean['cnt'] = df_clean['casual'] + df_clean['registered']
    print("Ajustes en 'cnt' por consistencia:", int((cnt_antes != df_clean['cnt']).sum()))

# Validar codificaciones categóricas y castear a int
def clamp_int_range(df_in, col, low, high):
    if col in df_in.columns:
        df_in[col] = pd.to_numeric(df_in[col], errors='coerce').round().astype('Int64')
        valid = df_in[col].between(low, high)
        dropped = int((~valid).sum())
        if dropped:
            print(f"Filas eliminadas por '{col}' fuera de [{low},{high}]: {dropped}")
        return df_in[valid]
    return df_in

df_clean = clamp_int_range(df_clean, 'season', 1, 4)
df_clean = clamp_int_range(df_clean, 'yr', 0, 1)
df_clean = clamp_int_range(df_clean, 'mnth', 1, 12)
df_clean = clamp_int_range(df_clean, 'holiday', 0, 1)
df_clean = clamp_int_range(df_clean, 'weekday', 0, 6)
df_clean = clamp_int_range(df_clean, 'workingday', 0, 1)
df_clean = clamp_int_range(df_clean, 'weathersit', 1, 4)

for col in ['season','yr','mnth','holiday','weekday','workingday','weathersit']:
    if col in df_clean.columns:
        df_clean[col] = df_clean[col].astype(int)

#Orden final
if 'dteday' in df_clean.columns:
    df_clean = df_clean.sort_values('dteday').reset_index(drop=True)

# RESÚMEN
print("\nShape final:", df_clean.shape)
print("\nInfo final del dataset:")
print(df_clean.info())
print("\nNulos por columna (post-limpieza):")
print(df_clean.isnull().sum())
print("\nVista previa depurada:")
print(df_clean.head(10))

# Comprobación rápida de consistencia:
if set(['casual','registered','cnt']).issubset(df_clean.columns):
    inconsist = int((df_clean['cnt'] != df_clean['casual'] + df_clean['registered']).sum())
    print(f"\nFilas con cnt inconsistente tras limpieza: {inconsist}")
else:
    print("\nNota: No están todas las columnas de conteo para verificar consistencia cnt.")

#  dataset limpio
df = df_clean.copy()
print("Listo: df limpio para continuar ->", df.shape)

temp = df['temp']

# MEDIA
print("Resultados de temp")
temp = df['temp']
media_temp = temp.mean()
print(f"Datos:{temp}")
print(f"Media:{media_temp}")
# MEDIANA
mediana_temp = temp.median()
print(f"Mediana:{mediana_temp}")
# MODA
from collections import Counter
conteo_py = Counter(temp)
print(conteo_py)
moda_temp = [k for k, v in conteo_py.items() if v == max(conteo_py.values())]
print(f"Moda: {moda_temp}")

# Graficos
plt.figure(figsize=(8, 5))
sns.histplot(temp, bins=5, kde=True)
plt.axvline(media_temp, color='r', linestyle='--', label=f'Media: {media_temp:.2f}')
plt.axvline(mediana_temp, color='g', linestyle='-', label=f'Mediana: {mediana_temp}')
for m in moda_temp:
    plt.axvline(m, color='b', linestyle=':', label=f'Moda: {m}')
plt.title('Distribución de Datos y Medidas de Tendencia Central (temp)')
plt.xlabel('Valor')
plt.ylabel('Frecuencia')
plt.legend()
plt.grid(axis='y', alpha=0.75)
plt.show()

atemp = df['atemp']

# MEDIA
media_atemp = atemp.mean()
print(f"Datos:{atemp}")
print(f"Media:{media_atemp}")
# MEDIANA
mediana_atemp = atemp.median()
print(f"Mediana:{mediana_atemp}")
#MODA
from collections import Counter
conteo2_py = Counter(atemp)
print(conteo2_py)
moda_atemp = [k for k, v in conteo2_py.items() if v == max(conteo2_py.values())]
print(f"Moda: {moda_atemp}")

# Graficos
plt.figure(figsize=(8,5))
sns.histplot(atemp, bins = 5, kde = True)
plt.axvline(media_atemp, color = 'r',linestyle='--',label=f'Media:{media_atemp:.2f}')
plt.axvline(mediana_atemp, color = 'g',linestyle='-',label=f'Mediana:{mediana_atemp}')
for m in moda_atemp:
  plt.axvline(m,color = 'b',linestyle=':',label=f'Moda:{m}')
plt.title('Distribución de Datos y Medidas de Tendencia Central (atemp)')
plt.xlabel('Valor')
plt.ylabel('Frecuencia')
plt.legend()
plt.grid(axis='y', alpha=0.75)
plt.show()

hum = df['hum']

# MEDIA
media_hum = hum.mean()
print(f"Datos:{hum}")
print(f"Media:{media_hum}")
# MEDIANA
mediana_hum = hum.median()
print(f"Mediana:{mediana_hum}")
# MODA
from collections import Counter
conteo3_py = Counter(hum)
print(conteo3_py)
moda_hum = [k for k, v in conteo3_py.items() if v == max(conteo3_py.values())]
print(f"Moda:{moda_hum}")

# Graficos

plt.figure(figsize=(8, 5))
sns.histplot(hum, bins=5, kde=True)
plt.axvline(media_hum, color='r', linestyle='--', label=f'Media: {media_hum:.2f}')
plt.axvline(mediana_hum, color='g', linestyle='-', label=f'Mediana: {mediana_hum}')
for m in moda_hum:
    plt.axvline(m, color='b', linestyle=':', label=f'Moda: {m}')
plt.title('Distribución de Datos y Medidas de Tendencia Central (hum)')
plt.xlabel('Valor')
plt.ylabel('Frecuencia')
plt.legend()
plt.grid(axis='y', alpha=0.75)
plt.show()

windspeed = df['windspeed']

# MODA
media_windspeed = windspeed.mean()
print(f"Datos:{windspeed}")
print(f"Datos:{media_windspeed}")
# MEDIANA
mediana_windspeed = windspeed.median()
print(f"Datos:{mediana_windspeed}")
# MODA
conteo4_py = Counter(windspeed)
print(conteo4_py)
moda_windspeed = [k for k, v in conteo4_py.items() if v == max(conteo4_py.values())]
print(f"Moda:{moda_windspeed}")

# Graficos

plt.figure(figsize=(8, 5))
sns.histplot(windspeed, bins=5, kde=True)
plt.axvline(media_windspeed, color='r', linestyle='--', label=f'Media: {media_windspeed:.2f}')
plt.axvline(mediana_windspeed, color='g', linestyle='-', label=f'Mediana: {mediana_windspeed}')
for m in moda_windspeed:
    plt.axvline(m, color='b', linestyle=':', label=f'Moda: {m}')
plt.title('Distribución de Datos y Medidas de Tendencia Central (windspeed)')
plt.xlabel('Valor')
plt.ylabel('Frecuencia')
plt.legend()
plt.grid(axis='y', alpha=0.75)
plt.show()

# MEDIDAS DE DISPERSION (RANGO, VARIANZA, DESVIACION ESTANDAR)

datos_dispersion_temp = df['temp']

# RANGO
rango_temp = np.max(datos_dispersion_temp) - np.min(datos_dispersion_temp)
print(f"Datos: {datos_dispersion_temp}")
print(f"Rango: {rango_temp}")
# VARIANZA
varianza_temp = np.var(datos_dispersion_temp)
print(f"Varianza: {varianza_temp:.2f}")
# DESVIACION ESTANDAR
desviacion_temp = np.std(datos_dispersion_temp)
print(f"Desviacion estandar: {desviacion_temp:.2f}")
# GRAFICO
plt.figure(figsize=(8,5))
sns.histplot(datos_dispersion_temp, bins=5, kde=True)
plt.axvline(np.mean(datos_dispersion_temp), color='r',linestyle='--',label=f'Media:{np.mean(datos_dispersion_temp):.2f}')
plt.axvspan(np.mean(datos_dispersion_temp)- np.std(datos_dispersion_temp), np.mean(datos_dispersion_temp) + np.std(datos_dispersion_temp), color = 'gray', alpha=0.2,label='±1 Desviacion Estándar')
plt.title('Distribución de Datos y Medidas de Dispersión (temp)')
plt.xlabel('Valor')
plt.ylabel('Frecuencia')
plt.grid(axis='y',alpha=0.75)
plt.show()

datos_dispersion_atemp = df['atemp']

# RANGO
rango_atemp = np.max(datos_dispersion_atemp) - np.min(datos_dispersion_atemp)
print(f"Datos: {datos_dispersion_atemp}")
# VARIANZA
varianza_atemp = np.var(datos_dispersion_atemp)
print(f"Datos: {varianza_atemp:.2f}")
# DESVIACION ESTANDAR
desviacion_atemp = np.std(datos_dispersion_atemp)
print(f"Desviacion estandar: {desviacion_atemp:.2f}")
# GRAFICO
plt.figure(figsize=(8,5))
sns.histplot(datos_dispersion_atemp, bins=5, kde=True )
plt.axvline(np.mean(datos_dispersion_atemp), color='r', linestyle='--', label=f'Media: {np.mean(datos_dispersion_atemp):.2f}')
plt.axvspan(np.mean(datos_dispersion_atemp) - np.std(datos_dispersion_atemp), np.mean(datos_dispersion_atemp) + np.std(datos_dispersion_atemp), color='gray', alpha=0.2, label='±1 Desviación Estándar')
plt.title('Distribución de Datos y Medidas de Dispersión (Atemp)')
plt.xlabel('Valor')
plt.ylabel('Frecuencia')
plt.legend()
plt.grid(axis='y', alpha=0.75)
plt.show()

datos_dispersion_hum = df['hum']

# RANGO
rango_hum = np.max(datos_dispersion_hum) - np.min(datos_dispersion_hum)
print(f"Datos: {datos_dispersion_hum}")
print(f"Rango: {rango_hum}")
# VARIANZA
varianza_hum = np.var(datos_dispersion_hum)
print(f"Varianza: {varianza_hum:.2f}")
# DESVIACION ESTANDAR
desviacion_hum = np.std(datos_dispersion_hum)
print(f"Desviacion estandar: {desviacion_hum:.2f}")
# GRAFICO
plt.figure(figsize=(8,5))
sns.histplot(datos_dispersion_hum, bins=5, kde=True )
plt.axvline(np.mean(datos_dispersion_hum),color='r', linestyle='--', label=f'Media: {np.mean(datos_dispersion_hum):.2f}')
plt.axvspan(np.mean(datos_dispersion_hum) - np.std(datos_dispersion_hum), np.mean(datos_dispersion_hum) + np.std(datos_dispersion_hum), color='gray', alpha=0.2, label='±1 Desviación Estándar')
plt.title('Distribución de Datos y Medidas de Dispersión (Hum)')
plt.xlabel('Valor')
plt.ylabel('Frecuencia')
plt.legend()
plt.grid(axis='y', alpha=0.75)
plt.show()

datos_dispersion_windspeed = df['windspeed']

# RANGO
rango_windspeed = np.max(datos_dispersion_windspeed) - np.min(datos_dispersion_windspeed)
print(f"Datos: {datos_dispersion_windspeed}")
print(f"Rango: {rango_windspeed}")
# VARIANZA
varianza_windspeed = np.var(datos_dispersion_windspeed)
print(f"Varianza: {varianza_windspeed:.2f}")
# DESVIACION ESTANDAR
desviacion_windspeed = np.std(datos_dispersion_windspeed)
print(f"Desviacion estandar: {desviacion_windspeed:.2f}")
cnt = df['cnt']
# GRAFICO
plt.figure(figsize=(8,5))
sns.histplot(datos_dispersion_windspeed, bins=5, kde=True )
plt.axvline(np.mean(datos_dispersion_windspeed),color='r', linestyle='--', label=f'Media: {np.mean(datos_dispersion_windspeed):.2f}')
plt.axvspan(np.mean(datos_dispersion_windspeed) - np.std(datos_dispersion_windspeed), np.mean(datos_dispersion_windspeed) + np.std(datos_dispersion_windspeed), color='gray', alpha=0.2, label='±1 Desviación Estándar')
plt.title('Distribución de Datos y Medidas de Dispersión (Windspeed)')
plt.xlabel('Valor')
plt.ylabel('Frecuencia')
plt.legend()
plt.grid(axis='y', alpha=0.75)
plt.show()


print("\n--- Estadísticas de cnt (uso total de bicicletas) ---")

# MEDIA
media_cnt = cnt.mean()
print(f"Media: {media_cnt:.2f}")
# MEDIANA
mediana_cnt = cnt.median()
print(f"Mediana: {mediana_cnt}")
# MODA
from collections import Counter
conteo_cnt = Counter(cnt)
moda_cnt = [k for k, v in conteo_cnt.items() if v == max(conteo_cnt.values())]
print(f"Moda: {moda_cnt}")
# GRAFICO
plt.figure(figsize=(8, 5))
sns.histplot(cnt, bins=5, kde=True)
plt.axvline(media_cnt, color='r', linestyle='--', label=f'Media: {media_cnt:.2f}')
plt.axvline(mediana_cnt, color='g', linestyle='-', label=f'Mediana: {mediana_cnt}')
for m in moda_cnt:
    plt.axvline(m, color='b', linestyle=':', label=f'Moda: {m}')
plt.title('Distribución de Datos y Medidas de Tendencia Central (Cnt)')
plt.xlabel('Valor')
plt.ylabel('Frecuencia')
plt.legend()
plt.grid(axis='y', alpha=0.75)
plt.show()

datos_dispersion_cnt = df['cnt']

# RANGO
rango_cnt = np.max(cnt) - np.min(cnt)
print(f"Datos: {datos_dispersion_cnt}")
print(f"Rango: {rango_cnt}")
# VARIANZA
varianza_cnt = np.var(cnt)
print(f"Varianza: {varianza_cnt:.2f}")
# DESVIACIÓN ESTÁNDAR
desviacion_cnt = np.std(cnt)
print(f"Desviación estándar: {desviacion_cnt:.2f}")
# GRAFICO
plt.figure(figsize=(8,5))
sns.histplot(datos_dispersion_cnt, bins=5, kde=True )
plt.axvline(np.mean(datos_dispersion_cnt),color='r', linestyle='--', label=f'Media: {np.mean(datos_dispersion_cnt):.2f}')
plt.axvspan(np.mean(datos_dispersion_cnt) - np.std(datos_dispersion_cnt), np.mean(datos_dispersion_cnt) + np.std(datos_dispersion_cnt), color='gray', alpha=0.2, label='±1 Desviación Estándar')
plt.title('Distribución de Datos y Medidas de Dispersión (Cnt)')
plt.xlabel('Valor')
plt.ylabel('Frecuencia')
plt.legend()
plt.grid(axis='y', alpha=0.75)
plt.show()

# ANALISIS ESTADISTICO BASICO
# PREGUNTA PRINCIPAL
# ¿Qué variables explican mejor el uso diario de bicicletas?
print("En el datSe analizaron 4 condiciones ambientales: temperatura real, temperatura sentida, humedad relativa y velocidad del viento respecto al uso de bicicletas")
correlacion = df[['temp','atemp','hum','windspeed','cnt']].corr()
print(f"correlacion: {correlacion}")
print("Hay una relación positiva moderada-alta: cuando la temperatura sube, también sube el uso de bicicletas. \nEsto indica que la gente prefiere usar bici en días más cálidos.")
# PREGUNTA DE APOYO 1
# ¿Cómo afectan la temperatura, la humedad y el viento al número total de bicicletas utilizadas?
print("Correlación temp-cnt:", df['temp'].corr(df['cnt']))
print("Correlación atemp-cnt:", df['atemp'].corr(df['cnt']))
print("\nDispersión:")
print("Desviación estándar temp:", df['temp'].std())
print("Desviación estándar atemp:", df['atemp'].std())
print("Temperatura: correlación positiva \n Humedad: correlación negativa \nViento: correlación negativa")
# PREGUNTA DE APOYO 3
# ¿Existen diferencias en el uso de bicicletas entre días con condiciones favorables y desfavorables?
df['condicion'] = df.apply(lambda row: 'Favorable'
                           if (0.3 <= row['temp'] <= 0.7 and row['hum'] < 0.6 and row['windspeed'] < 0.3)
                           else 'Desfavorable', axis=1)
print("\nFilas Favorables:")
print(df[df['condicion'] == 'Favorable'])
print("\nFilas Desfavorables:")
print(df[df['condicion'] == 'Desfavorable'])

# Identificacion de outliers

columnas = ['temp', 'hum', 'windspeed', 'cnt']

for col in columnas:
    datos = df[col]
# Calcular cuartiles
Q1 = np.percentile(datos, 25)
Q3 = np.percentile(datos, 75)
IQR = Q3 - Q1

# Límites
lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR

# Outliers
outliers = datos[(datos < lower) | (datos > upper)]
print(f"\n Variable: {col}")
print(f"Q1: {Q1}, Q3: {Q3}, IQR: {IQR}")
print(f"Límite inferior: {lower}, Límite superior: {upper}")
print(f"Outliers detectados:\n{outliers.values}")

plt.figure(figsize=(8,5))
sns.boxplot(y=datos)
plt.title((f'Box Plot de Datos y Cuartiles{col}'))
plt.ylabel('col')
plt.grid(axis='y',alpha=0.75)
plt.show()


#3JJJJ
# Análisis estadístico más profundo  y 7) Modelado de los datos

# --- Importar librerías ---
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import statsmodels.api as sm
from statsmodels.stats.diagnostic import het_breuschpagan
from statsmodels.stats.stattools import durbin_watson
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.graphics.gofplots import qqplot
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

#  DataFrame limpio

if 'df' in globals():
    _df = df.copy()
elif 'df_clean' in globals():
    _df = df_clean.copy()
else:
    raise NameError("No se encontró 'df' ni 'df_clean'. Ejecuta primero la celda de carga/limpieza.")

assert 'cnt' in _df.columns, "Falta la columna objetivo 'cnt' en el DataFrame."

# Variables explicativas (sin objetivo/fecha/ID); solo numéricas
X_all = (
    _df.drop(columns=[c for c in ['cnt','dteday','instant'] if c in _df.columns])
       .select_dtypes(include=[np.number])
       .copy()
)
y_all = _df['cnt'].copy()

# ================================
#  Análisis estadístico más profundo
# ================================

#  Normalidad, asimetría y curtosis (variables) ---
desc_skew = X_all.skew(numeric_only=True).round(3)
desc_kurt = X_all.kurtosis(numeric_only=True).round(3)
print("\nAsimetría y curtosis (variables explicativas):")
print(pd.DataFrame({"skew": desc_skew, "kurtosis_exceso": desc_kurt}).head(12))

# Diagnóstico OLS (residuos) ---
X_ols = sm.add_constant(X_all)
ols = sm.OLS(y_all, X_ols).fit()
resid = ols.resid
fitted = ols.fittedvalues

print("\nResumen OLS (para diagnóstico):")
print(ols.summary())

# Normalidad de residuos
sh_stat, sh_p = stats.shapiro(resid)
print(f"\nNormalidad residuos (Shapiro-Wilk): stat={sh_stat:.3f}, p={sh_p:.3f} (p>0.05 sugiere normalidad)")
print(f"Asimetría residuos: {stats.skew(resid):.3f}")
print(f"Curtosis (exceso) residuos: {stats.kurtosis(resid):.3f}")

# Homocedasticidad e independencia
bp_stat, bp_p, _, _ = het_breuschpagan(resid, X_ols)
print(f"\nBreusch–Pagan (homocedasticidad): p={bp_p:.3f} (p>0.05 sugiere varianza constante)")
dw = durbin_watson(resid)
print(f"Durbin–Watson (independencia): {dw:.3f} (~2 indica baja autocorrelación)")

# Colinealidad (VIF)
vif = pd.DataFrame({
    "variable": X_ols.columns,
    "VIF": [variance_inflation_factor(X_ols.values, i) for i in range(X_ols.shape[1])]
}).sort_values("VIF", ascending=False)
print("\nVIF (colinealidad):")
print(vif.head(12))

# Gráficos de diagnóstico
plt.figure(figsize=(7,5))
plt.scatter(fitted, resid, alpha=0.6)
plt.axhline(0, color='r', linestyle='--', linewidth=1)
plt.title("Residuos vs. Valores ajustados (OLS)")
plt.xlabel("Valores ajustados")
plt.ylabel("Residuos")
plt.grid(alpha=0.3)
plt.show()

plt.figure(figsize=(6,6))
qqplot(resid, line='45', fit=True)
plt.title("QQ-plot de residuos (OLS)")
plt.grid(alpha=0.3)
plt.show()

# (Opcional) Selección de variables
usar_kbest = False
k = min(8, X_all.shape[1])
if usar_kbest and k > 0:
    sel = SelectKBest(score_func=f_regression, k=k).fit(X_all, y_all)
    selected_vars = X_all.columns[sel.get_support()].tolist()
    print("\nVariables seleccionadas (SelectKBest):", selected_vars)
    X_all = X_all[selected_vars]
else:
    print("\nFeature selection: desactivada (usar_kbest=False).")

# ================================
# Modelado de los datos
# ================================
X = X_all.copy()
y = y_all.copy()

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=42
)

# ---- Modelo 1: Regresión Lineal ----
lin = LinearRegression().fit(X_train, y_train)
pred_lin = lin.predict(X_test)

lin_mae  = mean_absolute_error(y_test, pred_lin)
lin_mse  = mean_squared_error(y_test, pred_lin)
lin_rmse = np.sqrt(lin_mse)             # <-- compatible con sklearn antiguos
lin_r2   = r2_score(y_test, pred_lin)

# CV (usar neg_mean_squared_error y luego raíz para compatibilidad)
cv_mse_lin = -cross_val_score(lin, X, y, scoring="neg_mean_squared_error", cv=5)
cv_rmse_lin = np.sqrt(cv_mse_lin)

# ---- Modelo 2: Árbol de Decisión ----
tree = DecisionTreeRegressor(random_state=42, max_depth=5, min_samples_leaf=5).fit(X_train, y_train)
pred_tree = tree.predict(X_test)

tree_mae  = mean_absolute_error(y_test, pred_tree)
tree_mse  = mean_squared_error(y_test, pred_tree)
tree_rmse = np.sqrt(tree_mse)           # <-- compatible con sklearn antiguos
tree_r2   = r2_score(y_test, pred_tree)

cv_mse_tree = -cross_val_score(tree, X, y, scoring="neg_mean_squared_error", cv=5)
cv_rmse_tree = np.sqrt(cv_mse_tree)

# ---- Resultados ----
print("\n=== Resultados (hold-out 20%) ===")
print(f"Regresión Lineal -> MAE: {lin_mae:.2f} | RMSE: {lin_rmse:.2f} | R²: {lin_r2:.3f}")
print(f"Árbol Decisión   -> MAE: {tree_mae:.2f} | RMSE: {tree_rmse:.2f} | R²: {tree_r2:.3f}")

print("\n=== Validación cruzada (5-fold) | RMSE (↓ mejor) ===")
print(f"Regresión Lineal -> {cv_rmse_lin.mean():.2f} ± {cv_rmse_lin.std():.2f}")
print(f"Árbol Decisión   -> {cv_rmse_tree.mean():.2f} ± {cv_rmse_tree.std():.2f}")

# Interpretabilidad básica
coef = pd.Series(lin.coef_, index=X.columns).sort_values(key=np.abs, ascending=False)
print("\nTop coeficientes (Regresión Lineal):")
print(coef.head(10))

imp = pd.Series(tree.feature_importances_, index=X.columns).sort_values(ascending=False)
print("\nTop importancias (Árbol de Decisión):")
print(imp.head(10))